{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1325e7df",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "\n",
    "Let $D = \\{X, y\\}$ be the collected data, where $X \\in \\mathbb{R}^{n \\times p}$ is the design matrix with full rank and $y \\in \\mathbb{R}^n$ is the vector of response. Consider the following optimization problem \\begin{equation} \\label{eq:problem_1} \\hat{\\beta} = \\arg\\min\\{\\lVert b \\rVert_2: b \\text{ minimizes } \\dfrac{1}{2n}(\\lVert y - Xb \\rVert_2)^2\\}. \\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8de0ec0",
   "metadata": {},
   "source": [
    "#### 1(a)\n",
    "\n",
    "Show that the optimal solution of Equation \\ref{eq:problem_1} is $\\hat{\\beta} = (X^T X)^{-1} X^T y$ when $n \\geq p$, and $\\hat{\\beta} = X^T (XX^T)^{-1} y$ when $n < p$. What is the degrees of freedom based on Stein’s lemma: $df = E\\left[\\sum_i \\dfrac{\\partial \\hat{y_i}}{\\partial y_i}\\right]$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7077ffc8",
   "metadata": {},
   "source": [
    "First let $k = \\min\\{n, p\\}$. Unless specified, the norms $\\lVert \\rVert$ in this problem refers to the $2$-norm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875eb9b0",
   "metadata": {},
   "source": [
    "Note $\\beta$ minimizes $\\lVert y - X b \\rVert^2$ iff $X\\beta$ is the orthogonal projection of $y$ onto the column space of $X$ as a subspace $U$ of $\\mathbb{R}^m$ since $\\lVert X\\beta \\rVert^2 + \\lVert y - X\\beta \\rVert^2 = \\lVert y \\rVert^2$, so there is a unique $X\\beta$ that minimize the distance from $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a73cc1b",
   "metadata": {},
   "source": [
    "Then we denote the singular value decomposition of $X = VDU^T$, where $D = \\text{diag}(\\lambda_1, \\cdots, \\lambda_k, 0, \\cdots, 0)$. Define $D^+ = \\text{diag}({\\lambda_1}^{-1}, \\cdots, {\\lambda_k}^{-1}, 0, \\cdots, 0)$, the pseudo-inverse of $X$ is defined as $X^+ = UD^+ V^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c35b001",
   "metadata": {},
   "source": [
    "We claim the solution of the problem $\\hat{\\beta} = \\text{arg} \\min_b \\lVert y - Xb \\rVert^2$ is given by $\\beta^+ = X^+ y = UD^+ V^T y$. Therefore, this $\\beta^+$ also solves the problem $\\text{arg} \\min_b (\\lVert y - Xb \\rVert^2)/(2n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed90e4c",
   "metadata": {},
   "source": [
    "#### Proof\n",
    "\n",
    "Assume $D$ is rectangular and diagonal. If $X = D$, then $b = D^+ y = X^+ y$ minimizes $\\lVert y - X b\\rVert^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e249ad7",
   "metadata": {},
   "source": [
    "Otherwise, we may write $\\lVert y - X b\\rVert = \\lVert y - VDU^T b\\rVert = \\lVert V^T(y - VDU^T b) \\rVert = \\lVert V^T y - DU^T b \\rVert$ since ($V$ and therefore) $V^T$ is an isometry regarding the $2$-norm such that $V^T V = I$ and $\\lVert V^T x \\rVert = \\lVert x \\rVert$ for all $x \\in \\mathbb{R}^n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26e54e9",
   "metadata": {},
   "source": [
    "Also since $U$ is surjective, $\\lVert y - X b\\rVert$ is minimized iff $\\lVert D \\gamma - V^T y\\rVert$ is minimized, where $\\gamma = U^T b$. The solution to the latter condition is then $\\hat{\\gamma} = D^+ V^T y$. Then the solution to the problem $\\hat{\\beta} = \\text{arg}\\min_b \\lVert y - X b \\rVert^2 $ is given by $\\hat{\\beta} = UD^+ V^T y = X^+ y$;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85f21aa",
   "metadata": {},
   "source": [
    "Furthermore, the Moore-Penrose inverse of $X$ is given by \\begin{equation*} X^+ = \\begin{cases} (X^TX)^{-1} X^T & \\text{ when } X \\text{ has linearly independent columns, i.e. } n \\geq p, \\\\ XT(XX^T)^{-1} & \\text{ when } X \\text{ has linearly independent rows, i.e. } n < p, \\\\  \\end{cases} \\end{equation*} which fulfill Moore-Penrose conditions. Since the pseudo-inverse is unique, and the optimization problem is $\\text{arg}\\min_b \\lVert y - X b \\rVert^2$ divided by $2n$ (a constant), the optimal solution of the optimization problem is also given by $\\hat{\\beta} = X^+ y$. $\\Box$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c4988d",
   "metadata": {},
   "source": [
    "As for the degrees of freedom, note the prediction of $y$ is given by $\\hat{y} = X\\hat{\\beta}$. Then the degrees of freedom should be $\\text{trace}(XX^+) = \\begin{cases} \\text{trace}(X(X^TX)^{-1}X^T) & \\text{ if } n \\geq p \\\\ \\text{trace}(XX^T(XX^T)^{-1}) & \\text{ if } n < p  \\end{cases} = \\min\\{n, p\\} = k$. $\\Box$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516f89cd",
   "metadata": {},
   "source": [
    "#### 1(b)\n",
    "\n",
    "Mikhail Belkin et al. (2019) PNAS paper “Reconciling modern machine-learning practice and the classical bias–variance trade-off” demonstrated the double decent phenomenon for many machine learning methods. Let $\\gamma = p/n$. Can you use simulation study to demonstrate the double decent phenomenon with the above linear model in the underparameterized regime ($\\gamma$ < 1), overparameterized regime ($\\gamma$ > 1) and the special regime ($\\gamma$ = 1)?\n",
    "\n",
    "It would be great if you can show the pattern of bias-variance tradeoff in these different regimes. For example, you may use the value of $\\gamma$ as the $x$-axis, and use squared bias and variance as the y-axis to visualize the bias-variance tradeoff. Of course, the answer to this part is quite open."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c3e8b0",
   "metadata": {},
   "source": [
    "#### The generated sample\n",
    "\n",
    "A sample of size $n = 200$ was simulated from the following setting: \\begin{equation} Y = X_{200 \\times p} B_{p \\times 1} + R, \\end{equation} where $[X_{ij}] \\overset{i.i.d.}{\\sim} N(0, 0.25^2), \\beta_i \\overset{i.i.d.}{\\sim} N(0, 0.25^2), \\epsilon_i \\overset{i.i.d.}{\\sim} N(0, 1)$ for $i = 1, 2, \\cdots, 200$ and $j = 1, 2, \\cdots, p$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff12a17a",
   "metadata": {},
   "source": [
    "Only $\\epsilon$s are considered random when fitting the OLS regression model. $n$ is fixed at 200 while we try to change the value of $p$ from 1 to 400 so we can observe if double decent would appear as we increase $p$. $70\\%$ data is assigned for training and the remaining $30\\%$ is for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81550a65",
   "metadata": {},
   "source": [
    "The following procedures are repeated for $R = 200$ times.\n",
    "\n",
    "For each dimension $p = 1, 2, \\cdots, 400$, we generate the design matrix whose elements are from $[X_{200 \\times p}]_{ij} \\overset{i.i.d.}{\\sim} N(0, 0.25^2)$. Since we keep changing the dimension of $X$, we cannot fix a certain design matrix. Then we predict $Y$ and call the prediction $\\hat{Y}$. Define $\\theta^{(r)}$ as the value in the $r^{\\text{th}}$ replication for any parameter $\\theta$. For the training data, the average bias (?) is calculated as \\begin{equation} \\overline{\\text{Bias}} = \\dfrac{1}{R}\\dfrac{1}{0.75n} \\sum_{i = 1}^{0.75n} \\sum_{r = 1}^R \\left({\\hat{y_i}}^{(r)} - y_i\\right).\\end{equation} The average variance is calculated as \\begin{equation} \\overline{\\text{Var}} = \\dfrac{1}{R}\\dfrac{1}{0.75n} \\sum_{i = 1}^{0.75n} \\sum_{r = 1}^R \\left({\\hat{y_i}}^{(r)} - \\dfrac{1}{R} \\sum_{s = 1}^{R} {\\hat{y_i}}^{(s)}\\right)^2.\\end{equation} For the testing data, the $0.75n$ is changed to $0.25n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19abf1c1",
   "metadata": {},
   "source": [
    "The code and the plots are shown below. The $y$-axis is the MSE, average variance, average bias for both training and testing data, while the $x$-axis is $\\gamma = p/n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ef9b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linspace as lins\n",
    "from numpy.random import normal, default_rng\n",
    "from numpy.linalg import pinv\n",
    "from sklearn.model_selection import train_test_split as TTS\n",
    "import time\n",
    "\n",
    "def workflow(repeats, test_ratio):\n",
    "\n",
    "    n = 200  # sample size\n",
    "    ps = lins(1, 2 * n, 2 * n)  # endpoint=True\n",
    "\n",
    "    mu_x, mu_b, mu_e, sigma_x, sigma_b, sigma_e = 0, 0.2, 0, 0.2, 0, 1\n",
    "    var_x, var_b, var_e = sigma_x ** 2, sigma_b ** 2, sigma_e ** 2\n",
    "\n",
    "    bias_train, vars_train, bias_test, vars_test =\\\n",
    "        [0] * len(ps), [0] * len(ps), [0] * len(ps), [0] * len(ps)\n",
    "\n",
    "    for p in ps:\n",
    "\n",
    "        gamma = p / n\n",
    "        train_bias, train_vars, test_bias, test_vars = [], [], [], []\n",
    "\n",
    "        for i in range(repeats):\n",
    "\n",
    "            rng = default_rng(i+1)\n",
    "            X = rng.normal(mu_x, var_x, (n, int(p)))  # X\n",
    "            B = rng.normal(mu_b, var_b, (int(p), 1))  # Beta\n",
    "            R = rng.normal(mu_e, var_e, (n, 1))       # eRRoR\n",
    "\n",
    "            # Underlying model\n",
    "            Y = X @ B + R  # matrix product\n",
    "\n",
    "            # Train test split\n",
    "            X_train, X_test, Y_train, Y_test = TTS(\n",
    "                X, Y, test_size = tr, random_state = 42)\n",
    "\n",
    "            # Estimate the parameter beta_i in B\n",
    "            if gamma <= 1:  # i.e. n >= p\n",
    "                B_pred = pinv(X_train.transpose() @ X_train) @ X_train.transpose() @ Y_train\n",
    "            else:           # i.e. n < p\n",
    "                B_pred = X_train.transpose() @ pinv(X_train @ X_train.transpose()) @ Y_train\n",
    "\n",
    "            Y_train_pred = X_train @ B_pred\n",
    "            Y_test_pred = X_test @ B_pred\n",
    "\n",
    "            train_bias.append((sum((Y_train_pred - Y_train) ** 2) /\n",
    "                               ((1 - tr) * n))[0])\n",
    "            train_vars.append((sum((Y_train_pred - sum(Y_train_pred) /\n",
    "                                    repeats) ** 2) / ((1 - tr) * n))[0])\n",
    "            test_bias.append((sum((Y_test_pred - Y_test) ** 2) / (tr * n))[0])\n",
    "            test_vars.append((sum((Y_test_pred - sum(Y_test_pred) /\n",
    "                                    repeats) ** 2) / (tr * n))[0])\n",
    "\n",
    "        # for each p\n",
    "        bias_train[int(p)-1] = sum(train_bias) / (repeats)\n",
    "        bias_test[int(p)-1] = sum(test_bias) / (repeats)\n",
    "        vars_train[int(p)-1] = sum(train_vars) / (repeats)\n",
    "        vars_test[int(p)-1] = sum(test_vars) / (repeats)\n",
    "\n",
    "    return bias_train, bias_test, vars_train, vars_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c30ce34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['font.family'] = 'Arial'\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15a557e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.perf_counter()\n",
    "\n",
    "# workflow\n",
    "tr_1b = 0.3 # test to total ratio\n",
    "\n",
    "# the main statement\n",
    "bias_train, bias_test, vars_train, vars_test = workflow(\n",
    "    repeats=200, test_ratio=tr_1b)\n",
    "\n",
    "end = time.perf_counter()\n",
    "\n",
    "print(\"The workflow takes {} seconds!\".format(end - start))\n",
    "\n",
    "theps = lins(1, 40, 40)/20\n",
    "max_1b = max(max(bias_train), max(bias_test), max(vars_train), max(vars_test))\n",
    "\n",
    "figa, axea = plt.subplots()\n",
    "axea.set_xlabel('p/n')\n",
    "axea.set_ylabel('Bias squared or Variance')\n",
    "axea.plot(theps, bias_train, label=\"Train bias sq\")\n",
    "axea.plot(theps, bias_test, label=\"Test bias sq\")\n",
    "axea.plot(theps, vars_train, label=\"Train vars\")\n",
    "axea.plot(theps, vars_test, label=\"Test vars\")\n",
    "axea.plot([1 - tr, 1 - tr], [0, max_1b], 'k--', label=\"Train to all ratio\")\n",
    "axea.legend()\n",
    "plt.xlim(min(theps), max(theps))\n",
    "plt.ylim(0, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abda2db3",
   "metadata": {},
   "source": [
    "It's actually interesting to observe that given $t_r$ being the size ratio of the test set to the whole data set, which equals 0.3 in this simulation study, the peak of the test set bias squared and variance is located around $\\lambda = 1 - t_r = 0.7$. Before that, both quantities go up quick, and go down a bit slower with a few flucuations. In contrast, the training bias squared goes down to near 0 and the training variance goes up steadily to around 1 as $\\lambda$ approaches $1 - t_r$, and remain almost constant all the way after. This figure shows the double descent phenomena bewteen the training bias squared and test bias squared, which looks similar to Figure 4 about the fully connected neural network on MNIST of the original paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8311aa8a",
   "metadata": {},
   "source": [
    "#### 1(c)\n",
    "Intialize $\\beta^{(0)} = 0$, and gradient descent on the least square loss yields \\begin{equation} \\label{eq:beta_updates}\n",
    "    \\beta^{(k)} = \\beta^{(k-1)} + \\dfrac{\\epsilon}{n} X^T (y - X\\beta^{(k-1)}), \\end{equation}\n",
    "where we take $0 < \\epsilon \\leq 1/\\lambda_{\\max, X^T X/n}$.\n",
    "Will the gradient descent converge to the optimal solution given in (a)? Please justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba85d585",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf4936da",
   "metadata": {},
   "source": [
    "#### 1(d)\n",
    "Given the gradient flow DE for the LS problem -- $\\min (\\lVert y - Xb \\rVert_2)^2/(2n)$: \\begin{equation} \\label{eq:dbeta_dt}\n",
    "    \\dfrac{d\\beta(t)}{dt} = \\dfrac{X^T(y - X\\beta(t))}{n},\n",
    "\\end{equation} what is the exact solution path $\\beta(t)$ to Eq. \\ref{eq:dbeta_dt} for all $t$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736abb13",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65959bf3",
   "metadata": {},
   "source": [
    "#### 1(e)\n",
    "Use simulation study to investigate the differences between the solution of Ridge regression: \\begin{equation}\n",
    "    \\hat{\\beta}(\\lambda) = (X^T X + n \\lambda I)^{-1} X^T y\n",
    "\\end{equation} and the solution of the gradient flow $\\hat{\\beta}(t)$. You may compare the similarity of their solution paths and their prediction accuracies along the solution paths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8b1fe7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef575156",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2aaa4ea6",
   "metadata": {},
   "source": [
    "### Problem 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b100f844",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4bc4db68",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7db730d3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36479dcf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2e78574",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0f01b8f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "906caa62",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cdc0dfde",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a418b29",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a60c4ed",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81e883bb",
   "metadata": {},
   "source": [
    "#### 2(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e794a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28281cb7",
   "metadata": {},
   "source": [
    "#### 2(c)\n",
    "\n",
    "We can observe that when the model is misspecified, the MSE for both MLE and JSE are both larger than that when the model is correct. In both cases, JSE performed much better than MLE from the boxplot. Comparing the MSE ratio of misspecified and correct model (*???* vs *???*), the ratio in the correct model is higher. Here is a summary: \\begin{itemize}\n",
    "\\item When the model is correctly specified, the JSE performs much better than MLE.\n",
    "\\item When the model is incorrectly specified, the MSE will be larger on average. The JSE still performs\n",
    "better than MLE though the efficiency may be affected by the misclassification.\n",
    "\\en{itemize}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c215707",
   "metadata": {},
   "source": [
    "### Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727956b8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b5ab298",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5459d089",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ec17cb2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8bf3519",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f67dd38",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
